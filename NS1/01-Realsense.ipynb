{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# NS1 - Realsense\n",
    "## Depth Estimation and Depth Sensors\n",
    "Goal is to estimate the distance between objects and pixels in the image plane. With no prior knowledge about the objects in the scene we cannot estimate this from one monocular image only.\n",
    "<figure style=\"text-align:center\">\n",
    "    <img src=\"assets/depth_sample_nyu.jpeg\" />\n",
    "    <figcaption > Indoor Depth Image (from NYU dataset) </figcaption>\n",
    "</figure>\n",
    "\n",
    "There are different approaches to estimate the depth, depending on which sensors are used in the process. \n",
    "\n",
    "### Time of Flight Sensors (ToF Sensors)\n",
    "<figure style=\"text-align:center\">\n",
    "    <center> <img src=\"assets/PPHAU-Time-of-Flight-Sensors.png\" /> </center>\n",
    "    <figcaption > ToF sensors</figcaption>\n",
    "</figure>\n",
    "\n",
    "* Estimate the depth by by measuring the time it takes for a light puls to reflect of target.\n",
    "* An example of this would be Azure Kinect Sensor.\n",
    "\n",
    "### Stereo Depth Sensors\n",
    "<figure style=\"text-align:center\">\n",
    "    <center><img style=\"allignt:center\" src=\"assets/stereo-ssd-1.png\" /></center>\n",
    "    <figcaption > Stereo Depth Reconstruction (image source https://www.intelrealsense.com/stereo-depth-vision-basics/)</figcaption>\n",
    "</figure>\n",
    "\n",
    "* Estimate the depth using two infrared/color cameras with on the same baseline with a known displacement between them. \n",
    "* Match pixels/blocks between images using epipolar line search.\n",
    "* An example of that is Intel Realsense D435i.\n",
    "\n",
    "## Realsense D435(i): Tools and Setup\n",
    "\n",
    "We are going to mainly use this sensor in both multiview tabletop setup, and head-mounted setup. So it is important to have a good understanding it, and the limitations the sensor has.\n",
    "\n",
    "D435 and D435i sensors have stereo infrared depth sensor with static laser pattern for active stereo.\n",
    "Sensor specs from the [Intel-RealSense-D400-Series-Datasheet](https://www.intel.com/content/dam/support/us/en/documents/emerging-technologies/intel-realsense-technology/Intel-RealSense-D400-Series-Datasheet.pdf)\n",
    "\n",
    "* 1280x720 active stereo depth resolution\n",
    "* 1920x1080 RGB resolution\n",
    "* Depth Diagonal Field of View over 90°\n",
    "* Dual global shutter sensors for up to 90 FPS depth streaming\n",
    "* Range 0.2m to over 10m (Varies with lighting conditions)\n",
    "* D435i includes Inertial Measurement Unit (IMU) for 6 degrees of freedom (6DoF) data.\n",
    "\n",
    "\n",
    "There are multiple [whitepapers](https://dev.intelrealsense.com/docs/whitepapers) available to show the performace and limitation of these sensors. We will not cover them in this project lab.\n",
    "\n",
    "### [realsense-viewer](https://github.com/IntelRealSense/librealsense/tree/master/tools/realsense-viewer)\n",
    "A software which allows visualizing and recording and playing recorded sequences. \n",
    "Good for recording small sequences and experimenting with different post processing filters.\n",
    "We will use it to do intrinsics and stereo extrinsics calibration.\n",
    "<figure style=\"text-align:center\">\n",
    "    <center><img src=\"assets/realsense-viewer.png\" /> </center>\n",
    "    <figcaption> Realsense Viewer </figcaption>\n",
    "</figure>\n",
    "More on calibration and post-processing filters later.\n",
    "\n",
    "#### Setup\n",
    "Installation for realsense SDK and Viewer could be found in the [github repo](https://github.com/IntelRealSense/librealsense)\n",
    "\n",
    "### [ROS Wrapper](https://github.com/IntelRealSense/realsense-ros)\n",
    "A ROS package which supports multiple applications like streaming color, depth, and point clouds in addition to other examples like SLAM (along with tracking sensors).\n",
    "<figure style=\"text-align:center\">\n",
    "    <img src=\"assets/rs-pointcloud-rviz.png\" />\n",
    "    <figcaption> Rviz visualization of rs-pointcloud launch</figcaption>\n",
    "</figure>    \n",
    "We will mainly use rosbags to record ego-perspective, and multiview sequences.\n",
    "\n",
    "#### Setup\n",
    "\n",
    "We are using [ROS Noetic](https://wiki.ros.org/noetic) setup the desktop-full. \n",
    "The packages we need are in https://github.com/IntelRealSense/realsense-ros for ubuntu they you could install the pre-built packages.\n",
    "\n",
    "```bash\n",
    "apt install ros-noetic-realsense2-camera ros-noetic-realsense2-camera-dbgsym ros-noetic-realsense2-description\n",
    "```\n",
    "\n",
    "#### Ros Messages\n",
    "When reading the data from a RowWrapper rosbag or directly from the sensor there are some important topics depending on the [launch file](https://github.com/IntelRealSense/realsense-ros/tree/development/realsense2_camera/launch)\n",
    "\n",
    "\n",
    "\n",
    "| topic  | type  | description  | launch file  | required options |\n",
    "|---|---|---|---| --- |\n",
    "| /camera/color/image_raw| sensor_msgs/Image  |  contains raw color image | rs_camera.launch  |  enable_color:=true |  \n",
    "| /camera/aligned_depth_to_color/image_raw  | sensor_msgs/Image  | contains the depth image aligned  | rs_camera.launch | align_depth:=true |\n",
    "| /camera/infra1/image_rect_raw | sensor_msgs/Image | contains rectified left infra image | enable_infra1:=true |\n",
    "| /camera/infra2/image_rect_raw | sensor_msgs/Image | contains rectified right infra image | enable_infra2:=true |\n",
    " \n",
    "\n",
    "#### Camera Relative Transformation\n",
    "The sensor has two infrared cameras and one color. Therefore, we have multiple frames for each of the cameras. These relative transformations are important to get relative transformation from depth to color for example.\n",
    "<figure style=\"text-align:center\">\n",
    "    <center><img src=\"assets/tf_static.svg\" /> </center>\n",
    "    <figcaption> Camera Transformation </figcaption>\n",
    "</figure>\n",
    "\n",
    "### [SDK (python/c++)](https://github.com/IntelRealSense/librealsense)\n",
    "A software development kit which supports reading data from device and recordings (rosbags).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Perspective Projection\n",
    "<!-- ![Pinhole-Camera-Model-ideal-projection-of-a-3D-object-on-a-2D-image.png](attachment:8492ac5b-f710-4bd5-a110-d055626774a7.png)## Pinhole Camera Model -->\n",
    "<figure style=\"text-align:center\">\n",
    "    <img src=\"assets/perspective_projection.png\" />\n",
    "    <figcaption > Pinhole camera and Perspective Projection </figcaption>\n",
    "</figure>\n",
    "\n",
    "The equation to project 3D points into the camera plane\n",
    "\n",
    "\n",
    "$$pixels = K \\times [R|t] \\times Points$$\n",
    "\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "u \\\\\n",
    "v \\\\\n",
    "1\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "f_x & 0 & c_x \\\\\n",
    "0 & f_y & c_y \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "r_{0,0} & r_{0,1} & r_{0,2} & t_x \\\\\n",
    "r_{1,0} & r_{1,1} & r_{1,2} & t_y\\\\\n",
    "r_{2,0} & r_{2,1} & r_{2,2} & t_z\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x\\\\\n",
    "y\\\\\n",
    "z\\\\\n",
    "1\n",
    "\\end {bmatrix}\n",
    "$$\n",
    " \n",
    "### Camera Intrinsics\n",
    "These are parameters specific to the camera physichal model \n",
    "* $c_x, c_y$ are the principal point coordinates \n",
    "* $f$ focal length / principal point $z$ value\n",
    "* $f_x$ focal length $f$ $/$ pixel width\n",
    "* $f_y$ focal length $f$ $/$ pixel height\n",
    "* Calibration matrix 3x3 matrix $$K = \n",
    "    \\begin{bmatrix}\n",
    "        f_x & 0 & c_x \\\\\n",
    "       0 & f_y & c_y \\\\\n",
    "       0 & 0 & 1 \\end{bmatrix}$$\n",
    "* Camera Extrinsics a 4x3 matrix $T=[R|t]$\n",
    "\n",
    "$$ T = \\begin{bmatrix}\n",
    "r_{0,0} & r_{0,1} & r_{0,2} & t_0\\\\\n",
    "r_{1,0} & r_{1,1} & r_{1,2} & t_1\\\\\n",
    "r_{2,0} & r_{2,1} & r_{2,2} & t_2\\\\\n",
    "\\end{bmatrix}$$\n",
    "* Distortion Model\n",
    "Depending on lens shape the difference between real lens shape\n",
    "D435i has \"plumb bob\"/\"brown conrady\" distortion model, which means it has two types of distortion [7.](https://calib.io/blogs/knowledge-base/camera-models)\n",
    "  * Radial Distortion: Since the lens has a circular shape\n",
    "  * Tangential Distortion: The image seems tilted and stretched because different lens elements not beeing perfectly aligned, or because the optical axis is not perfectly normal to the sensor plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use pyrender library to render images from a camera while controlling its intrinsics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa26b4f6d94645eaaed049b0f3c34a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=400.0, description='f', max=2000.0, min=-2000.0, step=0.01), FloatSlid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import pyrender\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "\n",
    "# inspired by https://pyrender.readthedocs.io/en/latest/examples/quickstart.html\n",
    "model_path = 'assets/004_sugar_box/textured.obj'\n",
    "trimesh_model = trimesh.load(model_path)\n",
    "model = pyrender.Mesh.from_trimesh(trimesh_model)\n",
    "scene = pyrender.Scene()\n",
    "object_pose = np.eye(4)\n",
    "object_pose[:3,3] = [0,0,2]\n",
    "camera = pyrender.IntrinsicsCamera(fx=640, fy=640, cx=200, cy=200)\n",
    "camera_pose = np.eye(4)\n",
    "camera_pose[:3,3] = [0.3, 0.0, 0.35]\n",
    "camera_pose[:3,:3] = Rotation.from_euler('xyz', [45, 0, 90],degrees=True).as_matrix()\n",
    "scene.add(camera, pose=camera_pose)\n",
    "light = pyrender.SpotLight(color=np.ones(3), intensity=50.0,\n",
    "                                innerConeAngle=np.pi/16.0,\n",
    "                                outerConeAngle=np.pi/6.0)\n",
    "scene.add(light, pose=camera_pose)\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "scene.add(model)\n",
    "\n",
    "def update_camera(f:float, mx:int,my:int,cx:float, cy:float, height:int,width: int):\n",
    "    global camera\n",
    "    fx = f/mx\n",
    "    fy = f/my\n",
    "    camera.fx = fx\n",
    "    camera.fy = fy\n",
    "    camera.cx = cx\n",
    "    camera.cy = cy\n",
    "    \n",
    "    r = pyrender.OffscreenRenderer(width,height)\n",
    "    color, depth = r.render(scene)\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    # plt.axis('off')\n",
    "    plt.imshow(color)\n",
    "    plt.title('color')\n",
    "    plt.subplot(1,2,2)\n",
    "    # plt.axis('off')\n",
    "    plt.imshow(depth, cmap=plt.cm.gray_r)\n",
    "    plt.title('depth');\n",
    "    \n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "interact(update_camera, f=widgets.FloatSlider(value=400, min=-2000, max=2000, step=0.01),\n",
    "                        mx=widgets.FloatSlider(value=1, min=0.1, max=100,step=0.01),\n",
    "                        my=widgets.FloatSlider(value=1, min=0.1, max=100,step=0.01),\n",
    "                        cx=widgets.FloatText(value=320),\n",
    "                        cy=widgets.FloatText(value=240),\n",
    "                        height=widgets.FloatText(value=480),\n",
    "                        width=widgets.FloatText(value=640));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stereo Reconstruction\n",
    "<figure style=\"text-align:center\">\n",
    "    <img src=\"assets/realsense-stereo.png\" />\n",
    "    <figcaption > Stereo Reconstruction and Disparity Map </figcaption>\n",
    "</figure>\n",
    "\n",
    "### Epipolar Line Search and Disparity\n",
    "* Our goal is to find the matching pixels from left and right image. If we know this, we could use triangles similarity to estimate the depth.\n",
    "* The projection ray $C_{left}-P$ is a line on the right image plane called the epipolar line ($P_{right}, e_{right}$). \n",
    "* The intersection of the baeline and the image plane is called an epipole $e_{left}, e_{right}$\n",
    "* This can be shown in the figure below\n",
    "<figure style=\"text-align:center\">\n",
    "    <img src=\"assets/EpipolarLineSearch.png\" />\n",
    "    <figcaption > Epipolar Line Search </figcaption>\n",
    "</figure>\n",
    "\n",
    "* This means in order to find the matching pixel  for $P$ on the right view we only need to search on the Epipolar line corresponding to the ray $C_{left}-P$.\n",
    "\n",
    "* In other words, the epipolar constraints will reduce the search space for us to find matching pixels.\n",
    "\n",
    "* If the relative transformation between both views is only a horizontal displacement (i.e. cameras are alinged to the same basline). Then the the projection of $C_{left}-P$ to the right view view will be on the same row of pixels as in the left image (the epipoles will be in infinity because the baseline will be parallel to the pixels row).\n",
    "\n",
    "* In practice the relative cameras are not exactly alligned on the same baseline, but have a sligt rotation. Therefore, the process of projecting the image view into a view alligned with the baseline is known as rectification.\n",
    "\n",
    "* Note: In realsense the ros-topics rectified image topics has the suffix `_rect_raw`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<figure style=\"text-align:center\">\n",
    "    <img src=\"assets/realsense-stereo.png\" />\n",
    "    <figcaption > (https://dev.intelrealsense.com/docs/stereo-depth-cameras-for-phones) </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D435(i) Calibration\n",
    "The during calibration we optimize a subset of the sensor parameters to enhance the depth estimation. Realsense provides calibration using `realsense-viewer` and `dynamic calibration tool`. We will not cover them in details in this course.\n",
    "With realsense-viewer we could calibrate the following:\n",
    "* On Chip Calibration (stereo camera extrinsics)\n",
    "* Focal length Calibration (focal length)\n",
    "* Tare Calibration (stereo camera extrinsics)\n",
    "\n",
    "For more details check reference [9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "### Noe: Data for tasks 1 and 2 could be found [here](https://syncandshare.lrz.de/getlink/fiLmDyv8FXqFyN1X3hbhwazH/01-Realsense)\n",
    "\n",
    "### 1.Stereo Reconstruction and laser-pattern (workload 1 student):\n",
    "In this exercise, we will have a look over the \n",
    "1. Read the color, infrared1, infrared2 images in the folder Homework/HW-1-data (images with numbers (1262, 1755, 1131, 0000))\n",
    "2. Use OpenCV Stereo Block Matching to find the disparity map, then use the equation for depth to calculate the estimated depth map. You could assume that (focal_length=970 mm, baseline=50 mm) \n",
    "3. Use OpenCV to visualize the reconstructed depth image along with the infrared images using `cv2.imshow`.\n",
    "4. What is the difference between the depth quality with respect to \n",
    "     1. planes with texture (Checkerboard) vs. planes without texture (the PC case)\n",
    "     2. with laser pattern (1262,1755) vs no laser-pattern (0000,1131) \n",
    "\n",
    "### 2. Object Twin (workload 3 students):\n",
    "In this exercise, we will load a realsense-viewer rosbag recording, then use opencv and pyrender to create a twin of a moving checkerboard.\n",
    "1. Loading color and depth data:\n",
    "     * Use pyrealsense2 to read the bagfile and acquire color, depth, aligned depth to color, color camera intrinsics, depth camera intrinsics. (Show the images in a loop using `cv2.imshow`)\n",
    "     \n",
    "2. Checkerboard detection and tracking: \n",
    "     * The checkerboard has a `6x9` pattern where each square has an edge length of 4 cm.\n",
    "     * Using opencv we want Find its corners (use `cv2.findChessboardCorners`, and `cv2.cornersSubPix`). then use `cv2.drawChessboardCorners` to overlay the detections on the colored image\n",
    "     * From the previous step, you will have 2D/3D correspondences for the corners. Use `cv2.solvePnP` to estimate the object to camera translation and rotation vectors.\n",
    "     * *Extra:* Use opencv drawing utils and perspective projection function to draw a 3D axis, and a cropping mask for the board. Useful functions here could be `cv2.line,cv2.projectPoints,cv2.fillPoly`.\n",
    "3. Modeling the checkerboard in pyrender:\n",
    "    * Using pyrender create a scene with camera and a `Box` mesh corresponding to the checkerboard.\n",
    "    * Notes:\n",
    "      1. You will need to scale the box and shift its center to match the checkerboard 3d coordinate system in opencv\n",
    "      2. To convert from opencv camera to pyrender camera in you system you may need to rotate your objects by 90 degees around the X-axis (depending on your implementation) \n",
    "4. Visualization:\n",
    "    * In the loop, update the mesh pose with the updated pose of the checkerboard\n",
    "    * Compare the rendered depth value to the actual algined_depth values we got from realsense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Resources\n",
    "[1]. https://www.intelrealsense.com/stereo-depth-vision-basics/\n",
    "\n",
    "[2]. https://dev.intelrealsense.com/docs/intel-realsensetm-d400-series-calibration-tools-user-guide\n",
    "\n",
    "[3]. https://dev.intelrealsense.com/docs/whitepapers\n",
    "\n",
    "[4]. https://docs.opencv.org/4.x/\n",
    "\n",
    "[5]. https://pyrender.readthedocs.io/en/latest/examples/quickstart.html\n",
    "\n",
    "[6]. https://wiki.ros.org/noetic\n",
    "\n",
    "[7]. https://calib.io/blogs/knowledge-base/camera-models\n",
    "\n",
    "[8]. https://web.stanford.edu/class/cs231a/course_notes/03-epipolar-geometry.pdf\n",
    "\n",
    "[9]. https://dev.intelrealsense.com/docs/intel-realsensetm-d400-series-calibration-tools-user-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
