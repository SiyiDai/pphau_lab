{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "952f1ece",
   "metadata": {},
   "source": [
    "## Homework\n",
    "### Noe: Data for tasks 1 and 2 could be found [here](https://syncandshare.lrz.de/getlink/fiLmDyv8FXqFyN1X3hbhwazH/01-Realsense)\n",
    "\n",
    "### 2. Object Twin (workload 3 students):\n",
    "In this exercise, we will load a realsense-viewer rosbag recording, then use opencv and pyrender to create a twin of a moving checkerboard.\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4402757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "##               Read bag from file                ##\n",
    "#####################################################\n",
    "# First import library\n",
    "import pyrealsense2 as rs\n",
    "# Import Numpy for easy array manipulation\n",
    "import numpy as np\n",
    "# Import OpenCV for easy image rendering\n",
    "import cv2\n",
    "# Import argparse for command-line options\n",
    "import argparse\n",
    "# Import os.path for file path manipulation\n",
    "import os.path\n",
    "import copy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933627ba",
   "metadata": {},
   "source": [
    "1. Loading color and depth data:\n",
    "     * Use pyrealsense2 to read the bagfile and acquire color, depth, aligned depth to color, color camera intrinsics, depth camera intrinsics. (Show the images in a loop using `cv2.imshow`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6db7cfc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyrealsense2.pyrealsense2.pipeline_profile at 0x16e14182230>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read from Stereo Module\n",
    "bag_input = '../Homework/HW1-2-data/20220405_220626.bag'\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = rs.pipeline()\n",
    "\n",
    "# Create a config object\n",
    "config = rs.config()\n",
    "\n",
    "# Tell config that we will use a recorded device from file to be used by the pipeline through playback.\n",
    "rs.config.enable_device_from_file(config, bag_input)\n",
    "\n",
    "# Configure the pipeline to stream the depth stream\n",
    "# Change this parameters according to the recorded bag file resolution\n",
    "config.enable_stream(rs.stream.depth, 848, 480,rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color,rs.format.rgb8, 30)\n",
    "\n",
    "# Start streaming from file\n",
    "pipeline.start(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d08dae66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyrealsense2.pyrealsense2.pipeline_profile object at 0x0000016E1597B570>\n",
      "depth_profile:  <pyrealsense2.video_stream_profile: Depth(0) 848x480 @ 30fps Z16>\n",
      "color_profile:  <pyrealsense2.video_stream_profile: Color(0) 640x480 @ 30fps RGB8>\n",
      "depth_intrinsics: [ 848x480  p[426.785 234.17]  f[419.127 419.127]  Brown Conrady [0 0 0 0 0] ]\n",
      "color_intrinsics: [ 640x480  p[323.426 249.374]  f[607.323 606.302]  Inverse Brown Conrady [0 0 0 0 0] ]\n",
      "Width and Height: 848 480\n"
     ]
    }
   ],
   "source": [
    "# Create colorizer object\n",
    "colorizer = rs.colorizer()\n",
    "\n",
    "profile = pipeline.get_active_profile()\n",
    "print(profile)\n",
    "depth_profile = rs.video_stream_profile(profile.get_stream(rs.stream.depth))\n",
    "print(\"depth_profile: \",depth_profile)\n",
    "color_profile = rs.video_stream_profile(profile.get_stream(rs.stream.color))\n",
    "print(\"color_profile: \", color_profile)\n",
    "depth_intrinsics = depth_profile.get_intrinsics()\n",
    "color_intrinsics = color_profile.get_intrinsics()\n",
    "print('depth_intrinsics:', depth_intrinsics)\n",
    "print('color_intrinsics:', color_intrinsics)\n",
    "w, h = depth_intrinsics.width, depth_intrinsics.height\n",
    "print('Width and Height:',w,h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfdfc44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define detectors parameters\n",
    "pattern_size = (9, 6)\n",
    "board_pattern = [9, 6]\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "# Get camera matrix and distortion coefficients\n",
    "camera_matrix = np.array([[color_intrinsics.fx, 0, color_intrinsics.ppx], [0, color_intrinsics.fy, color_intrinsics.ppy], [0, 0, 1]])\n",
    "dist_coeffs = np.array([0., 0., 0., 0., 0.])\n",
    "\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((board_pattern[1] * board_pattern[0],3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:board_pattern[0],0:board_pattern[1]].T.reshape(-1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45a1593",
   "metadata": {},
   "source": [
    "2. Checkerboard detection and tracking: \n",
    "     * The checkerboard has a `6x9` pattern where each square has an edge length of 4 cm.\n",
    "     * Using opencv we want Find its corners (use `cv2.findChessboardCorners`, and `cv2.cornersSubPix`). then use `cv2.drawChessboardCorners` to overlay the detections on the colored image\n",
    "     * From the previous step, you will have 2D/3D correspondences for the corners. Use `cv2.solvePnP` to estimate the object to camera translation and rotation vectors.\n",
    "     * *Extra:* Use opencv drawing utils and perspective projection function to draw a 3D axis, and a cropping mask for the board. Useful functions here could be `cv2.line,cv2.projectPoints,cv2.fillPoly`.\n",
    "3. Modeling the checkerboard in pyrender:\n",
    "    * Using pyrender create a scene with camera and a `Box` mesh corresponding to the checkerboard.\n",
    "    * Notes:\n",
    "      1. You will need to scale the box and shift its center to match the checkerboard 3d coordinate system in opencv\n",
    "      2. To convert from opencv camera to pyrender camera in you system you may need to rotate your objects by 90 degees around the X-axis (depending on your implementation) \n",
    "4. Visualization:\n",
    "    * In the loop, update the mesh pose with the updated pose of the checkerboard\n",
    "    * Compare the rendered depth value to the actual algined_depth values we got from realsense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b0a84f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(img, corners, imgpts):\n",
    "    imgpts = np.int32(imgpts).reshape(-1,2)\n",
    "    # draw ground floor in green\n",
    "    img = cv2.drawContours(img, [imgpts[:4]],-1,(0,255,0),-3)\n",
    "    # draw pillars in blue color\n",
    "    for i,j in zip(range(4),range(4,8)):\n",
    "        img = cv2.line(img, tuple(imgpts[i]), tuple(imgpts[j]),(255),3)\n",
    "    # draw top layer in red color\n",
    "    img = cv2.drawContours(img, [imgpts[4:]],-1,(0,0,255),3)\n",
    "    return img\n",
    "\n",
    "objpoints = [] # 3d point in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "# axis = np.float32([[8,0,0], [0,5,0], [0,0,-3]]).reshape(-1,3)\n",
    "axis = np.float32([[0,0,0], [0,5,0], [8,5,0], [8,0,0],\n",
    "                   [0,0,-3],[0,5,-3],[8,5,-3],[8,0,-3] ])\n",
    "# Streaming loop\n",
    "while True:\n",
    "    # Get frameset of depth\n",
    "    frames = pipeline.wait_for_frames()\n",
    "\n",
    "    # Get depth frame\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "    \n",
    "    # Get color frame\n",
    "    color_frame = frames.get_color_frame()\n",
    "\n",
    "    \n",
    "    # Colorize depth frame to jet colormap\n",
    "    depth_color_frame = colorizer.colorize(depth_frame)\n",
    "\n",
    "    # Convert depth_frame to numpy array to render image in opencv\n",
    "    depth_color_image = np.asanyarray(depth_color_frame.get_data())\n",
    "    \n",
    "    # get color frame\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    color_image2 = cp.deepcopy(color_image)\n",
    "    # get gray frame\n",
    "    gray_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect Chessboard Corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray_image, board_pattern, None)\n",
    "    \n",
    "    if ret == True:\n",
    "        # Refining pixel coordinates for given 2d points.\n",
    "        corners2 = cv2.cornerSubPix(gray_image, corners, (11,11),(-1,-1), criteria)\n",
    "        objpoints.append(objp)\n",
    "        imgpoints.append(corners)\n",
    "        \n",
    "        # SolvePnP and project back2 2D\n",
    "        success, rotation_vector, translation_vector = cv2.solvePnP(objp, corners, camera_matrix, dist_coeffs, flags=0)\n",
    "        imgpoints2, _ = cv2.projectPoints(axis, rotation_vector, translation_vector, camera_matrix, dist_coeffs)\n",
    "        \n",
    "        # 2.1 Draw and display the corners\n",
    "        cv2.drawChessboardCorners(color_image, board_pattern, corners2, ret)\n",
    "        cv2.imshow('Corners', color_image)\n",
    "        \n",
    "        # 2.2 Draw 3D axis\n",
    "        corners2 = corners2.astype(int)\n",
    "        imgpoints2 = imgpoints2.astype(int)\n",
    "        color_image2 = draw(color_image2,corners2,imgpoints2) \n",
    "#         cv2.fillPoly(color_image2, pts = [contours], color =(0,255,0))\n",
    "        cv2.imshow(\"filledPolygon\", color_image2)\n",
    "        \n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    # if pressed escape exit program\n",
    "    if key == 27:\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "np.save('objpoints.npy', objpoints)\n",
    "np.save('imgpoints.npy', imgpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d6e487",
   "metadata": {},
   "source": [
    "## References and Resources\n",
    "[1]. https://www.intelrealsense.com/stereo-depth-vision-basics/\n",
    "\n",
    "[2]. https://dev.intelrealsense.com/docs/intel-realsensetm-d400-series-calibration-tools-user-guide\n",
    "\n",
    "[3]. https://dev.intelrealsense.com/docs/whitepapers\n",
    "\n",
    "[4]. https://docs.opencv.org/4.x/\n",
    "\n",
    "[5]. https://pyrender.readthedocs.io/en/latest/examples/quickstart.html\n",
    "\n",
    "[6]. https://wiki.ros.org/noetic\n",
    "\n",
    "[7]. https://calib.io/blogs/knowledge-base/camera-models\n",
    "\n",
    "[8]. https://web.stanford.edu/class/cs231a/course_notes/03-epipolar-geometry.pdf\n",
    "\n",
    "[9]. https://dev.intelrealsense.com/docs/intel-realsensetm-d400-series-calibration-tools-user-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c089443d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
