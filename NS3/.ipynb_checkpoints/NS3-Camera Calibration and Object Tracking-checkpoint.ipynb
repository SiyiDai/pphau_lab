{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3cd5f0c-154f-4235-871c-eb69006dcb65",
   "metadata": {},
   "source": [
    "# NS3-Camera Calibration and Object Tracking\n",
    "\n",
    "## Assets\n",
    "Download assets and homework data from https://syncandshare.lrz.de/getlink/fiNisuRxdcMKkwAWrywo3ALb/NS3-Camera%20Calibration%20and%20Object%20Tracking (password is pphau22)\n",
    "\n",
    "### Rigid Body Objects\n",
    "We define rigid bodies or rigid objects as objects which do not deform, therefore, we could define the position of each vertex using a translation and a rotation (6DoF transformation) applied to the pivot of the object. Translation $t \\in R^3$ and rotation $r \\in SO(3)$\n",
    "### Surface Representation\n",
    "* Point Clouds\n",
    "object surface is defined as a set of points in 3D\n",
    "* Meshes\n",
    "One way to define objects is as a list of vertices. To define surfaces we could define faces connceting these surfaces.\n",
    "* Implicit Representation (Not covered here)\n",
    "there are other ways to represent surfaces like SDF (Signed Distance Fields) or implicit representations using neural networks (NERF: Neural Radiance Fields [2]). Each representation has advantages and disatvantages depending on the operation we want to apply to it. For example pointclouds/meshes are fast to evaluate points, or traversing the surface. However, SDFs are more efficient for collision detection. [1]\n",
    "\n",
    "### Object Scanning and Processing\n",
    "\n",
    "In SS2 we shown objects scanned using Meshroom [4]. The final output of this software is a textured mesh that we could use for our own applications.\n",
    "Note that textured meshes define color on the mesh using a UV map that will be projected to wrap the mehs surface. To simplify our setup, we could use Meshlab[3] to transfer the colors from textures to vertices, also we could apply other transformations and filters like scaling, moving the origin of the object,...etc.\n",
    "\n",
    "\n",
    "Here we will use open3d [5] and show an example pointcloud/mesh loading and manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa35efdd-12cc-42de-aaf6-4ca4b63e712d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "# required libraries\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.transform.rotation import Rotation\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eabf97ec-dcc5-49f9-bee1-ad80402939df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D INFO] Skipping non-triangle primitive geometry of type: 2\n",
      " is textured? True. has vertex colors? False. num_vertices = 376811. num_faces(triangles) = 688594\n"
     ]
    }
   ],
   "source": [
    "# load mesh from .ply or .obj file\n",
    "# if you have a textured mesh (colors are defined using a uv map we should enable post processing enable_post_processing=True)\n",
    "textured_mesh = o3d.io.read_triangle_mesh('assets/models/oats/texturedMesh_alligned.obj', enable_post_processing=True)\n",
    "print(f\" is textured? {textured_mesh.has_textures()}. has vertex colors? {textured_mesh.has_vertex_colors()}. num_vertices = {len(textured_mesh.vertices)}. num_faces(triangles) = {len(textured_mesh.triangles)}\")\n",
    "# visualize the data (blocking)\n",
    "o3d.visualization.draw_geometries([textured_mesh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf703690-bd13-4010-92bc-1b67b2680bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is textured? False. has vertex colors? True. num_vertices = 344438. num_faces(triangles) = 688606\n"
     ]
    }
   ],
   "source": [
    "mesh = o3d.io.read_triangle_mesh('assets/models/oats/texturedMesh_alligned_vertex_color.ply')\n",
    "print(f\" is textured? {mesh.has_textures()}. has vertex colors? {mesh.has_vertex_colors()}. num_vertices = {len(mesh.vertices)}. num_faces(triangles) = {len(mesh.triangles)}\")\n",
    "object_transform = np.eye(4)\n",
    "object_transform[:3,:3] = Rotation.from_euler('xyz',[45,0,90], degrees=True).as_matrix()\n",
    "object_transform[:3, 3] = [1,2,4]\n",
    "# add axis coordinate frame (xyz - rgb)\n",
    "mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.5, origin=[0,0,0])\n",
    "mesh.transform(object_transform)\n",
    "mesh_frame.transform(object_transform)\n",
    "\n",
    "# visualize the data (blocking)\n",
    "o3d.visualization.draw_geometries([mesh, mesh_frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "533acd0f-4828-469a-8681-2f0d2ed66285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voxel_down_sampling: num points before downsampleing is 344438, after is 344073\n",
      "num points before downsampleing is 344438, after is 34444\n"
     ]
    }
   ],
   "source": [
    "# Processing point clouds\n",
    "pcd = o3d.io.read_point_cloud('assets/models/oats/texturedMesh_alligned_vertex_color.ply')\n",
    "'''\n",
    "# To convert from a trimesh:  (if you have a textured meshmake sure to transfer texture color to vertex color using Meshlab filters)\n",
    "assert mesh.has_vertex_colors\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points=mesh.vertices\n",
    "pcd.colors=mesh.vertex_colors\n",
    "'''\n",
    "voxel_down_sampled_pcd = o3d.geometry.PointCloud(pcd).voxel_down_sample(voxel_size=0.001).translate([1,0,0])\n",
    "print(f\"voxel_down_sampling: num points before downsampleing is {len(pcd.points)}, after is {len(voxel_down_sampled_pcd.points)}\")\n",
    "\n",
    "uniform_down_sampled_pcd = o3d.geometry.PointCloud(pcd).uniform_down_sample(every_k_points=10).translate([2,0,0])\n",
    "print(f\"num points before downsampleing is {len(pcd.points)}, after is {len(uniform_down_sampled_pcd.points)}\")\n",
    "\n",
    "\n",
    "scaled_pcd = o3d.geometry.PointCloud(pcd).scale(0.4, pcd.get_center()).translate([3,0,0])\n",
    "\n",
    "o3d.visualization.draw_geometries([pcd, voxel_down_sampled_pcd, uniform_down_sampled_pcd, scaled_pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9461d2b1-bd96-4c3a-9cb3-0f14b0e82c5f",
   "metadata": {},
   "source": [
    "## ICP (Iteractive Closest Point)\n",
    "Since, we use depth sensors as the base for this course, we also have access to depth data for every frame, and as a result an oriented pointcloud from aligned depth and color images. Therefore, we could use ICP algorithms this would allow us to utilize depth information to allign pointcloud.\n",
    "\n",
    "\n",
    "### Steps:\n",
    "#### Initial Conditions:\n",
    "We assume that we have 2 point clouds $P_{source}, Q_{target}$ and we want to register the source to the target by finding a rigid body transformation from source to target$T = [R|t]$. We assume that the rotation and translation between the the two pointclouds is small.\n",
    "\n",
    "#### Matching\n",
    "Every iteration, we find a set of $m$ correpondences ${(u_1,v_1), (u_2,v_2), ...}$ where $p_{u_i} \\in P_{source}$ and $q_{v_i} \\in Q_{target}$. \n",
    "The matches are selected bsaed on some cretirion. The simplest is to match based on the closest distance. Additionally, we could ignore outliers where the distance closest point distance is larger than a given threshold. \n",
    "\n",
    "We define the list of points $P_{source}^{inliers} = \\{p_{u_1}, ..., p_{u_m}\\}$ and $Q_{target}^{inliers} = \\{q_{v_1}, ..., q_{v_m}\\}$ \n",
    "\n",
    "#### Transformation Estimation Loss Minimization\n",
    "Our goal is to find $$T^* = argmin_{T} || Q_{target}^{inliers} - P_{source}^{inliers} \\times T  ||_2^2$$\n",
    "Where T = [R|t]. Since $R \\in SO(3)$ we need a constrained optimization scheme to guarantee that R is a valid rotation matrix (or a valid rotation representation in general Quaternion, AxisAngle,..). Finding $t$ is \n",
    "For more technichal details you could check [6]\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "* Visually we could visualize the transformed source point cloud and target to assist the quality of the result.\n",
    "* Quantitatively 2 metrics are used to evaluate the quality of the solution $T$\n",
    "    * Inlier RMSE root mean squared error for the correspondences (the lower the better) $RMSE_{inlier} =  \\sqrt{\\frac{|| Q_{target}^{inliers} - P_{source}^{inliers} \\times T  ||_2^2}{m}}$\n",
    "    * $Fitness = \\frac{m}{|Q_{target}|}$ (the higher the better) \n",
    "\n",
    "### Variations:\n",
    "### Point to Point ICP\n",
    "The approach we followed above is a point to point ICP. $$T^* = argmin_{T} || Q_{target}^{inliers} - P_{source}^{inliers} \\times T  ||_2^2$$\n",
    "### Point to Plane ICP\n",
    "The minimization objective here aims to push points the surface plane of the target point cloud, by minimizing the source point to target plane distance.\n",
    "This is done using the target point cloud normal vectors. $p_{u_i}$ belongs to the plane around \n",
    "$q_{v_i}$ if the dot product $(R \\times p_{u_i} + t - q_{v_i})n_{v_i} = 0$\n",
    "\n",
    "$N_{targe}^{inliers} = \\{ \\vec n_{v1},...,n_{v_m} \\}$\n",
    "\n",
    "$$T^* = argmin_{T} || (Q_{target}^{inliers} - P_{source}^{inliers} \\times T) \\times N_{target}^{inliers}  ||_2^2$$\n",
    "\n",
    "\n",
    "\n",
    "For more details you could check [5],[6] or [link](http://www.open3d.org/docs/latest/tutorial/t_pipelines/t_icp_registration.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a1d5fb-8ed5-4078-acdf-9fc17d55d125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77433877 0.78793649 1.50860303]\n",
      "Iteration registration errors: Inlier MSE is 0.04317638315654221, Inlier Fitness is 0.26397975826506503\n",
      "RMSE(same object) = 0.19948505394046923\n",
      "cumulative translation = [0.00054199 0.01852425 0.00673761] cumulative rotation = [-0.89804054 -0.02515301  0.09150917]\n",
      "Iteration registration errors: Inlier MSE is 0.04112175124982255, Inlier Fitness is 0.30274540933079486\n",
      "RMSE(same object) = 0.1982178458748994\n",
      "cumulative translation = [0.00104507 0.05382027 0.01899672] cumulative rotation = [-2.47771774 -0.08908557  0.28080563]\n",
      "Iteration registration errors: Inlier MSE is 0.038290566842299846, Inlier Fitness is 0.3442707332047553\n",
      "RMSE(same object) = 0.19540547651839169\n",
      "cumulative translation = [0.00066772 0.10017058 0.03452181] cumulative rotation = [-4.31189848 -0.20366529  0.55579727]\n",
      "Iteration registration errors: Inlier MSE is 0.03758221611088422, Inlier Fitness is 0.38756968949029774\n",
      "RMSE(same object) = 0.190971340920563\n",
      "cumulative translation = [-0.00056747  0.15160895  0.05123736] cumulative rotation = [-6.07445749 -0.33763632  0.87554511]\n",
      "Iteration registration errors: Inlier MSE is 0.03942410973158524, Inlier Fitness is 0.42566557726773785\n",
      "RMSE(same object) = 0.1853960460201539\n",
      "cumulative translation = [-0.00220765  0.20254926  0.0676701 ] cumulative rotation = [-7.50121654 -0.45772314  1.20716459]\n",
      "Iteration registration errors: Inlier MSE is 0.039876784311037, Inlier Fitness is 0.44059881799182643\n",
      "RMSE(same object) = 0.17911341898149072\n",
      "cumulative translation = [-0.00459516  0.25158825  0.08347988] cumulative rotation = [-8.50736831 -0.57912497  1.5486099 ]\n",
      "Iteration registration errors: Inlier MSE is 0.0398331926956328, Inlier Fitness is 0.45442818959262266\n",
      "RMSE(same object) = 0.17217937518397078\n",
      "cumulative translation = [-0.00743321  0.2987276   0.09838876] cumulative rotation = [-9.13668072 -0.70879038  1.88286389]\n",
      "Iteration registration errors: Inlier MSE is 0.04002415036555817, Inlier Fitness is 0.4672963268444847\n",
      "RMSE(same object) = 0.16512685161916946\n",
      "cumulative translation = [-0.01015485  0.3413005   0.11278128] cumulative rotation = [-9.24636412 -0.82471986  2.19057976]\n",
      "Iteration registration errors: Inlier MSE is 0.0402549773546507, Inlier Fitness is 0.4785830785545516\n",
      "RMSE(same object) = 0.15775767499390259\n",
      "cumulative translation = [-0.0124909   0.3777312   0.12667175] cumulative rotation = [-8.78199407 -0.91082788  2.50119367]\n",
      "Iteration registration errors: Inlier MSE is 0.040206816125947384, Inlier Fitness is 0.49056439960062265\n",
      "RMSE(same object) = 0.1501998734312777\n",
      "cumulative translation = [-0.01465811  0.4072601   0.13963382] cumulative rotation = [-7.70906613 -0.96239106  2.84125984]\n",
      "Iteration registration errors: Inlier MSE is 0.040432074260024216, Inlier Fitness is 0.5106883058089562\n",
      "RMSE(same object) = 0.1425667321931705\n",
      "cumulative translation = [-0.01653021  0.42902474  0.15079756] cumulative rotation = [-5.99736414 -0.98076718  3.22205652]\n",
      "Iteration registration errors: Inlier MSE is 0.04018731261618879, Inlier Fitness is 0.5323811945352277\n",
      "RMSE(same object) = 0.1351294731376246\n",
      "cumulative translation = [-0.01780079  0.44205731  0.15882464] cumulative rotation = [-3.60072704 -0.96786729  3.64915259]\n",
      "Iteration registration errors: Inlier MSE is 0.0398566318875498, Inlier Fitness is 0.5594135850320308\n",
      "RMSE(same object) = 0.12813458152085358\n",
      "cumulative translation = [-0.01807081  0.44548637  0.16195653] cumulative rotation = [-0.46918301 -0.91845923  4.12888108]\n",
      "Iteration registration errors: Inlier MSE is 0.03881612252743851, Inlier Fitness is 0.5902040917575705\n",
      "RMSE(same object) = 0.12181382989453061\n",
      "cumulative translation = [-0.01680878  0.43853462  0.15759258] cumulative rotation = [ 3.44445517 -0.83718926  4.6716731 ]\n",
      "Iteration registration errors: Inlier MSE is 0.03631188091975056, Inlier Fitness is 0.564554638420847\n",
      "RMSE(same object) = 0.11630334783712777\n",
      "cumulative translation = [-0.01351791  0.42322196  0.14542415] cumulative rotation = [ 7.50501278 -0.73205965  5.2172082 ]\n",
      "Iteration registration errors: Inlier MSE is 0.03913345239229795, Inlier Fitness is 0.5220866847337381\n",
      "RMSE(same object) = 0.1120611225260983\n",
      "cumulative translation = [-0.0087069   0.40461283  0.12837639] cumulative rotation = [10.91029441 -0.65111249  5.70337389]\n",
      "Iteration registration errors: Inlier MSE is 0.04216575889461061, Inlier Fitness is 0.5110727995485299\n",
      "RMSE(same object) = 0.1082473777210506\n",
      "cumulative translation = [-0.00140821  0.3827144   0.10710499] cumulative rotation = [13.48544155 -0.55253343  6.10064266]\n",
      "Iteration registration errors: Inlier MSE is 0.04197754040901698, Inlier Fitness is 0.5528275793638489\n",
      "RMSE(same object) = 0.10213285185716524\n",
      "cumulative translation = [0.00914664 0.35714731 0.08133686] cumulative rotation = [15.1831691  -0.36974453  6.40986508]\n",
      "Iteration registration errors: Inlier MSE is 0.03844176136443433, Inlier Fitness is 0.6628113934177152\n",
      "RMSE(same object) = 0.09198979597143442\n",
      "cumulative translation = [0.02303766 0.32387843 0.04616835] cumulative rotation = [16.44682079 -0.07037496  6.67263768]\n",
      "Iteration registration errors: Inlier MSE is 0.03397721262734908, Inlier Fitness is 0.8550892707642123\n",
      "RMSE(same object) = 0.07703830534041604\n",
      "cumulative translation = [ 0.0378471   0.27778373 -0.00681794] cumulative rotation = [18.35268467  0.29489536  6.89889721]\n",
      "Iteration registration errors: Inlier MSE is 0.024394754213498696, Inlier Fitness is 0.9849489305492679\n",
      "RMSE(same object) = 0.059423764212243435\n",
      "cumulative translation = [ 0.04788505  0.2301141  -0.07215091] cumulative rotation = [21.36871072  0.54548924  7.05446175]\n",
      "Iteration registration errors: Inlier MSE is 0.025444582427547175, Inlier Fitness is 0.999937984880714\n",
      "RMSE(same object) = 0.04817357569400054\n",
      "cumulative translation = [ 0.05071373  0.20532811 -0.12530699] cumulative rotation = [24.26101591  0.57318223  7.19946445]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m errors2 \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[0;32m---> 45\u001b[0m     reg_p2p \u001b[38;5;241m=\u001b[39m \u001b[43mo3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipelines\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregistration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregistration_icp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mpcd_solution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpcd_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_transformation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m                \u001b[49m\u001b[43mo3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipelines\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregistration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTransformationEstimationPointToPoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m                \u001b[49m\u001b[43mo3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipelines\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregistration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mICPConvergenceCriteria\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     solution_transform \u001b[38;5;241m=\u001b[39m reg_p2p\u001b[38;5;241m.\u001b[39mtransformation\n\u001b[1;32m     51\u001b[0m     cumulative_transformation \u001b[38;5;241m=\u001b[39m cumulative_transformation \u001b[38;5;241m@\u001b[39m solution_transform\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.spatial.transform.rotation import Rotation\n",
    "# read source point cloud\n",
    "pcd_source = o3d.io.read_point_cloud('assets/models/oats/texturedMesh_alligned_vertex_color.ply')\n",
    "pcd_source = pcd_source.voxel_down_sample(0.005)\n",
    "# visualize the source point cloud\n",
    "o3d.visualization.draw_geometries([pcd_source])\n",
    "# create a mock target point cloud as a toy example\n",
    "points = np.array(pcd_source.points)\n",
    "extents = points.max(axis=0) - points.min(axis=0)\n",
    "print(extents)\n",
    "translation = 0.05 * extents\n",
    "transform = np.eye(4)\n",
    "transform[:3,3] = translation\n",
    "pcd_target = o3d.geometry.PointCloud(pcd_source)\n",
    "transform[:3,:3] = Rotation.from_euler('xyz', [25,0,0], degrees=True).as_matrix()\n",
    "transform[:3,3] = [0,0,0.1]\n",
    "pcd_target.transform(transform)\n",
    "\n",
    "# set color for source to red and target to blue (later solution is green)\n",
    "pcd_source.paint_uniform_color([1,0,0])\n",
    "pcd_target.paint_uniform_color([0,1,0])\n",
    "\n",
    "o3d.visualization.draw_geometries([pcd_source, pcd_target])\n",
    "\n",
    "# Note it is better to run iCP once with ICPConvergenceCriteria(max_iteration=num_iteration) but here we want to visualize each step.\n",
    "num_iterations = 30\n",
    "initial_transformation = np.eye(4)\n",
    "threshold = 0.05 * max(extents)\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window()\n",
    "ctr = vis.get_view_control()\n",
    "ctr.set_lookat([0, 0, 0])\n",
    "pcd_solution = o3d.geometry.PointCloud(pcd_source)\n",
    "pcd_solution.paint_uniform_color([0,0,1])\n",
    "geometries = [pcd_source, pcd_target, pcd_solution]\n",
    "o3d.visualization.draw_geometries(geometries)\n",
    "cumulative_transformation = np.eye(4)\n",
    "errors = []\n",
    "errors2 = []\n",
    "for iteration in range(num_iterations):\n",
    "    reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "                    pcd_solution, pcd_target, threshold, initial_transformation,\n",
    "                o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "                o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=1),)\n",
    "    \n",
    "    solution_transform = reg_p2p.transformation\n",
    "    cumulative_transformation = cumulative_transformation @ solution_transform\n",
    "    diff = (np.array(pcd_solution.points) - np.array(pcd_target.points))\n",
    "    # exact_rmse = np.mean(np.sum(diff ** 2, axis=1)**0.5)\n",
    "    exact_rmse = mean_squared_error(np.array(pcd_solution.points), np.array(pcd_target.points), squared=False)\n",
    "    errors.append(exact_rmse) # calculation could be slightly different from rmse in open3d\n",
    "    errors2.append(reg_p2p.inlier_rmse)\n",
    "    print(f\"Iteration registration errors: Inlier MSE is {reg_p2p.inlier_rmse}, Inlier Fitness is {reg_p2p.fitness}\")\n",
    "    print(f\"RMSE(same object) = {exact_rmse}\")\n",
    "    print(f\"cumulative translation = {cumulative_transformation[:3,3]} cumulative rotation = {Rotation.from_matrix(cumulative_transformation[:3,:3]).as_euler('xyz', degrees=True)}\")\n",
    "    initial_transformation = solution_transform\n",
    "    pcd_solution = pcd_solution.transform(solution_transform)\n",
    "    if iteration%5 == 0:\n",
    "        o3d.visualization.draw_geometries(geometries)\n",
    "\n",
    "\n",
    "plt.plot(np.arange(num_iterations), errors)\n",
    "plt.plot(np.arange(num_iterations), errors2)\n",
    "\n",
    "plt.xlabel('num iterations')\n",
    "plt.ylabel('error(RMSE)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786ffa6b-53cb-40b4-bf68-87a02bb88fa9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Multiview Camera Calibration\n",
    "### Motivation\n",
    "\n",
    "In a simplified setup where we deal only with Rigid body objects, Hand-Object interactions are subject to different types of occlusion Hand-Hand occlusion, self-occlusion, and Hand-Object occlusion. These types of occlusions increase the difficulty of detection and pose estimation tasks.\n",
    "\n",
    "Multiple approaches exist to overcome this issue. One of them is data-driven statistical priors, where a machine learning model learns to hallucinate the occluded parts of the interaction object or hand keypoints.\n",
    "\n",
    "Another solution is to record data in multiple views and fuse them into one world coordinate system. Therefore it is required to estimate the relative camera pose with respect to the world coordinate system. \n",
    "Note: Usually, the world coordinate system is either one of the cameras or defined with respect to each of them, for example, the center of the table in the tabletop setup. In general, if we have $K$ views, we need $K$ rigid body transformation matrices.\n",
    "\n",
    "We learned in the previous lecture about camera intrinsics and stereo extrinsics. In this lecture, we will focus on camera-camera extrinsics.  \n",
    "\n",
    "### Calibration Process\n",
    "In our setup we will use the tools provided by opencv to calibrate different views using a checkerboard\n",
    "\n",
    "\n",
    "### Finding Correspondences (SIFT points)\n",
    "How to find correspondences between two images for the same object from different camera angles. \n",
    "We could extract SIFT feature points and match the keypoints in images based on descriptors similarity.\n",
    "\n",
    "#### Homography (Application for SIFT points)\n",
    "Given two images $I_1, I_2$ for a textured planar surface (for example, a chessboard, or a table surface with a pattern on it). \n",
    "Define each plane on the surface by a set of feature points $P_1, P_2$.\n",
    "Homography is a structure perserving transformation (In our case we are interested in finding 6DoF rigid body transformation) as this will correspond to the relative  transformation between two camera poses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfdb0f8-f841-42b7-87a4-a39fff806b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.opencv.org/3.4/d1/de0/tutorial_py_feature_homography.html\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "MIN_MATCH_COUNT = 10\n",
    "img1 = cv2.imread('assets/image1.png',0)          # queryImage\n",
    "img2 = cv2.imread('assets/image2.png',0)          # trainImage\n",
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks = 50)\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "# store all the good matches as per Lowe's ratio test.\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff3bf6-118c-4943-b850-17b127a5c16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(good)>MIN_MATCH_COUNT:\n",
    "    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "    matchesMask = mask.ravel().tolist()\n",
    "    h,w = img1.shape\n",
    "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "    dst = cv2.perspectiveTransform(pts,M)\n",
    "    img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "else:\n",
    "    print( \"Not enough matches are found - {}/{}\".format(len(good), MIN_MATCH_COUNT) )\n",
    "    matchesMask = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c94ab3f-2687-4e05-9ff0-b04cd3073997",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                   singlePointColor = None,\n",
    "               \n",
    "                   matchesMask = matchesMask, # draw only inliers\n",
    "                   flags = 2)\n",
    "img3 = cv2.drawMatches(img1,kp1,img2,kp2,good,None,**draw_params)\n",
    "fig=plt.figure(figsize=(20,20))\n",
    "plt.imshow(img3, 'gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f3bea3-a3d0-4924-9aba-bbe1d7d528fe",
   "metadata": {},
   "source": [
    "## RANSAC (RANdom SAmple Consensus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b9c2d6-d767-44b1-9d68-c7e4e86cae1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model as linear_model\n",
    "np.random.seed(123)\n",
    "# generate points    \n",
    "num_points = 50\n",
    "alpha = 0.6\n",
    "beta = np.random.normal(0.0, 0.03, size=(num_points,1))\n",
    "\n",
    "\n",
    "X = np.linspace(0, 1.0, num_points).reshape(num_points,1)\n",
    "Y = alpha * X + beta\n",
    "\n",
    "# randomly create outliers\n",
    "inlier_ratio = 0.8\n",
    "outlier_ratio = 1-inlier_ratio\n",
    "\n",
    "inlier_mask = np.random.choice([True, False], size=(num_points,), p=[inlier_ratio, outlier_ratio])\n",
    "outlier_mask = ~inlier_mask\n",
    "Y[outlier_mask] += np.random.normal(1,1)\n",
    "\n",
    "plt.scatter(X[inlier_mask],Y[inlier_mask], color='blue', label='inliers')\n",
    "plt.scatter(X[outlier_mask],Y[outlier_mask], color='red', label='outliers')\n",
    "# print(X.shape, Y.shape)\n",
    "linear_regression = linear_model.LinearRegression()\n",
    "linear_regression.fit(X , Y)\n",
    "# print(X.shape, linear_regression.coef_.shape, linear_regression.intercept_.shape)\n",
    "plt.plot(X, linear_regression.predict(X), label=\"linear regression\")\n",
    "\n",
    "ransac_linear_regression = linear_model.RANSACRegressor()\n",
    "ransac_linear_regression.fit(X, Y)\n",
    "\n",
    "plt.plot(X, ransac_linear_regression.predict(X), label=\"ransac linear regression\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Linear Regression vs Ransac Linear Regression');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197c4c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06aaefe0",
   "metadata": {},
   "source": [
    "### Perspective N â€“ points problem (PnP) \n",
    "* Given $N$ 3D points in the world coordinate system $P_w \\in \\mathbb{R}^{N \\times 3}$.\n",
    "* You are given 2 different views with a caliberated camera intrinsics $K\\in \\mathbb{R}^{3 \\times 3}$) with the 2D pixel correspondence of the previous $N$ points $p_{c}, p_{c} \\in \\mathbb{R}^{N \\times 2}$\n",
    "* Find the relative rotation and translation between the two camera views $[R|t] \\in \\mathbb{R}^{4x3}$.\n",
    "* The perpective projection is formulated as follows using homogeneous coordinates\n",
    "$$\\overline{p_c} = K \\times [R|t] \\times \\overline{P_w}$$\n",
    "$$ \\overline{P_w} = (x_{P_w}, y_{P_w}, z_{p_w}, 1)^T$$\n",
    "$$ p_c = (u, v) = \\frac{X_{p_c}} {Z_{p_c}} , \\frac{Y_{p_c}}{Z_{p_c}} $$ \n",
    "* We could use RANSAC to have a more robust estimation using `cv2.solvePnPRansac`.\n",
    "\n",
    "### Camera Pose Estimation\n",
    "* We could use `cv2.calibrateCamera` or `cv2.solvePnP` (does not use RANSAC) along with the checkerboard pattern to calibrate multiple views.\n",
    "* scripts/multiview.py contains a visualization of the aligned pointclouds based on checkerboard calibration.\n",
    "* Note that in the more general case we could estimate the distortion polynomial ceffecients $D$ and intrinsics $K$.\n",
    "* Our goal is to minimize the reprojection error which is mean squared error between 2D correspondences and the perspective projection of the 3D correspondences.\n",
    "* The default "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd17d6a-1eae-47ea-bc74-a9fcaf53c4d9",
   "metadata": {},
   "source": [
    "## SLAM using D435i\n",
    "* SLAM is short for Self Localization And Mapping\n",
    "* The algorithm tries to simultanously\n",
    "    * Build a map for the envionment\n",
    "    * Estimate the trajectory of the camera with repsect to the environment\n",
    "* Realsense provides an integration with opensource ros package for SLAM.\n",
    "* Utilizes RGBD + IMU data in the D435i\n",
    "* Ego-Perspective Setup could utilize the SLAM algorithm available in [realsense ros-wrapper](https://github.com/IntelRealSense/realsense-ros/blob/development/realsense2_camera/launch/opensource_tracking.launch) \n",
    "* You could use the D435i open source tracking in ros\n",
    "\n",
    "```sh\n",
    "roslaunch realsense2_camera opensource_tracking.launch\n",
    "```\n",
    "* Chcek [7] for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b72384-d3db-41a7-b856-c024b473edf2",
   "metadata": {},
   "source": [
    "## Homework (ICP Tracker)\n",
    "* Workload 4 students\n",
    "* Deadline due in 1 week\n",
    "* The goal of this homework is to apply ICP between a point cloud from a scanned model (source) and a a point cloud from realsense recording (ros wrapper bag).\n",
    "#### 1. Loading the Model\n",
    "* Load the point cloud for the oats can using `open3d`\n",
    "* Downsample the pointcloud using voxel downsampling (set voxel size to 0.01)\n",
    "* Resulting output should looks as follow \n",
    "\n",
    "<img src=\"assets/homework/images/downsampled_pcd.png\" width=300 title=\"Downsampled Model Pointcloud\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13a325f5-14de-484d-921f-9102de4821d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "# required libraries\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.transform.rotation import Rotation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be7523b-6e80-4d94-99d2-c146c5092c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the point cloud for the oats can using open3d\n",
    "pcd = o3d.io.read_point_cloud('assets/models/oats/texturedMesh_alligned_vertex_color.ply')\n",
    "\n",
    "# Downsample the pointcloud using voxel downsampling (set voxel size to 0.01)\n",
    "voxel_down_sampled_pcd = o3d.geometry.PointCloud(pcd).voxel_down_sample(voxel_size=0.01).translate([1,0,0])\n",
    "\n",
    "# visualize the down sampled model\n",
    "o3d.visualization.draw_geometries([voxel_down_sampled_pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd7aa40-877d-4d38-afac-2e6c8420eda5",
   "metadata": {},
   "source": [
    "#### 2. Loading the realsense sequence\n",
    "* Read the color, depth, and camera info topics from the rosbag using `pyrosbag` library. \n",
    "* It is enough to use the first 50 color and first depth frames.\n",
    "* use `o3d.geometry.PointCloud.create_from_rgbd_image` to create a point cloud\n",
    "* Resulting output should looks as follow \n",
    "\n",
    "<img src=\"assets/homework/images/realsense_pcd.png\" width=300 title=\"Realsense Point Cloud\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df8cb360-47e3-4786-a942-2f3324dda730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2022-06-05 15:05:32,196 - topics - topicmanager initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera aligned_depth_to_color camera_info\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiview\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BagWrapper\n\u001b[1;32m      4\u001b[0m bagfile \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massets/homework/icp_tracking_oats.bag\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mBagWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbagfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/master/course/SS22/pphau/pphau_lab/NS3/scripts/multiview.py:248\u001b[0m, in \u001b[0;36mBagWrapper.__init__\u001b[0;34m(self, bagfile, num_calibration_images, limit, serials)\u001b[0m\n\u001b[1;32m    245\u001b[0m \t\t\t\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcams[serial][modality][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntrinsics\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mcamera\u001b[38;5;241m.\u001b[39mPinholeCameraIntrinsic(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mintrinsics)\n\u001b[1;32m    247\u001b[0m \t\t\u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_raw\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 248\u001b[0m \t\t\timage \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m'\u001b[39m], cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB) \u001b[38;5;28;01mif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmsg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    249\u001b[0m \t\t\t\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[serial][modality]\u001b[38;5;241m.\u001b[39mappend(image)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m serial \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserials:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Read the color, depth, and camera info topics from the rosbag using pyrosbag library.\n",
    "import rosbag\n",
    "from scripts.multiview import BagWrapper\n",
    "bagfile = \"assets/homework/icp_tracking_oats.bag\"\n",
    "data = BagWrapper(bagfile)\n",
    "# It is enough to use the first 50 color and first depth frames.\n",
    "# pcd_comb = data.load_frame(n=50)\n",
    "# use o3d.geometry.PointCloud.create_from_rgbd_image to create a point cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2125b145",
   "metadata": {},
   "source": [
    "#### 3. Pose Initialization\n",
    "\n",
    "* Use `o3d.visualization.VisualizerWithEditing` following the [tutorial](http://www.open3d.org/docs/latest/tutorial/visualization/interactive_visualization.html?highlight=visualizerwithediting) to get correspondences between the realsense pointcloud and the oats can model pointcloud.\n",
    "* Find a rigid body transformation and scale for the initial pose using `open3d.pipelines.registration.TransformationEstimationPointToPoint`\n",
    "    \n",
    "<img src=\"assets/homework/images/oats_correspondences.png\" title=\"Picking Model Correspondences\" width=200 /> \n",
    "\n",
    "<img src=\"assets/homework/images/realsense_correspondences.png\" title=\"Realsense Correspondences\" width=200 />\n",
    "* Allign the model pointcloud to the realsense pointcloud, and visualize the allignment\n",
    "\n",
    "<img src=\"assets/homework/images/model_pcd_allignemtn.png\" title=\"Aligned Output\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec2e8ba2-51d5-458e-8313-624c860aaa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77505299 0.78658115 1.50726811]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PointCloud with 48541 points."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcd_source = voxel_down_sampled_pcd\n",
    "\n",
    "points = np.array(pcd_source.points)\n",
    "extents = points.max(axis=0) - points.min(axis=0)\n",
    "print(extents)\n",
    "translation = 0.05 * extents\n",
    "transform = np.eye(4)\n",
    "transform[:3,3] = translation\n",
    "pcd_target = o3d.geometry.PointCloud(pcd_source)\n",
    "transform[:3,:3] = Rotation.from_euler('xyz', [25,0,0], degrees=True).as_matrix()\n",
    "transform[:3,3] = [0,0,0.1]\n",
    "pcd_target.transform(transform)\n",
    "\n",
    "# set color for source to red and target to blue (later solution is green)\n",
    "# pcd_source.paint_uniform_color([1,0,0])\n",
    "# pcd_target.paint_uniform_color([0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04453043-203e-4a06-9a75-1dfba93fbee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_points(pcd):\n",
    "    print(\"\")\n",
    "    print(\n",
    "        \"1) Please pick at least three correspondences using [shift + left click]\"\n",
    "    )\n",
    "    print(\"   Press [shift + right click] to undo point picking\")\n",
    "    print(\"2) After picking points, press 'Q' to close the window\")\n",
    "    vis = o3d.visualization.VisualizerWithEditing()\n",
    "    vis.create_window()\n",
    "    vis.add_geometry(pcd)\n",
    "    vis.run()  # user picks points\n",
    "    vis.destroy_window()\n",
    "    print(\"\")\n",
    "    return vis.get_picked_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "558a16be-2c33-4fdd-9256-7cfaff2cff8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1) Please pick at least three correspondences using [shift + left click]\n",
      "   Press [shift + right click] to undo point picking\n",
      "2) After picking points, press 'Q' to close the window\n",
      "\n",
      "\n",
      "1) Please pick at least three correspondences using [shift + left click]\n",
      "   Press [shift + right click] to undo point picking\n",
      "2) After picking points, press 'Q' to close the window\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m picked_id_source \u001b[38;5;241m=\u001b[39m pick_points(pcd_source)\n\u001b[1;32m      3\u001b[0m picked_id_target \u001b[38;5;241m=\u001b[39m pick_points(pcd_target)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(picked_id_source) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(picked_id_target) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(picked_id_source) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(picked_id_target))\n\u001b[1;32m      6\u001b[0m corr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(picked_id_source), \u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pick points from two point clouds and builds correspondences\n",
    "picked_id_source = pick_points(pcd_source)\n",
    "picked_id_target = pick_points(pcd_target)\n",
    "assert (len(picked_id_source) >= 3 and len(picked_id_target) >= 3)\n",
    "assert (len(picked_id_source) == len(picked_id_target))\n",
    "corr = np.zeros((len(picked_id_source), 2))\n",
    "corr[:, 0] = picked_id_source\n",
    "corr[:, 1] = picked_id_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5298e3ed-a6fd-4378-96f8-4ac3629767e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "    target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a793d-bcb8-4695-8288-3ac9cb434d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate rough transformation using correspondences\n",
    "print(\"Compute a rough transform using the correspondences given by user\")\n",
    "p2p = o3d.pipelines.registration.TransformationEstimationPointToPoint()\n",
    "trans_init = p2p.compute_transformation(pcd_source, pcd_target,\n",
    "                                        o3d.utility.Vector2iVector(corr))\n",
    "\n",
    "# point-to-point ICP for refinement\n",
    "print(\"Perform point-to-point ICP refinement\")\n",
    "threshold = 0.03  # 3cm distance threshold\n",
    "reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "    pcd_source, pcd_target, threshold, trans_init,\n",
    "    o3d.pipelines.registration.TransformationEstimationPointToPoint())\n",
    "draw_registration_result(pcd_source, pcd_target, reg_p2p.transformation)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428452e9-3bac-4004-ad95-85dc857f6b1a",
   "metadata": {},
   "source": [
    "#### 4. Pose Tracking\n",
    "\n",
    "* Iterate over the color and depth frames each timestep and run ICP to refine the previous model pose (use the previous pose as initial pose).\n",
    "* Visualize The change overtime (you could reuse the source code in this notebook to help you visualizing the data.\n",
    "<img src=\"assets/homework/images/icp_after_first_few_iterations.png\" width=300 title=\"ICP Tracking After Few Iterations (Color)\"/>\n",
    "<img src=\"assets/homework/images/icp_after_first_few_iterations_diff.png\" width=300 title=\"ICP Tracking After Few Iterations (Change)\"/>\n",
    "* Plot or print the inlier RMSE and Fitness overtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f8cf3a-6836-45a5-beb2-6e5c865eedd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51a62d46-11cd-49a7-8308-47aaad044e4e",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "[1]. http://graphics.stanford.edu/courses/cs468-10-fall/LectureSlides/04_Surface_Reconstruction.pdf\n",
    "\n",
    "[2]. https://www.matthewtancik.com/nerf\n",
    "\n",
    "[3]. https://www.meshlab.net/\n",
    "\n",
    "[4]. https://alicevision.org/\n",
    "\n",
    "[5]. http://www.open3d.org/docs/release/\n",
    "\n",
    "[6]. http://graphics.stanford.edu/courses/cs468-10-fall/LectureSlides/03_Surface_Reconstruction.pdf\n",
    "\n",
    "[7].https://docs.opencv.org/4.4.0/d9/d0c/group__calib3d.html\n",
    "\n",
    "[8]. https://www.youtube.com/watch?v=tcJHnHpwCXk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffa288a-d24c-4a74-9124-e7d1511caf32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
